{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdffbfec",
   "metadata": {},
   "source": [
    "# Send and Retrieve vitals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199b43ba",
   "metadata": {},
   "source": [
    "This script facilitates remote processing of validation datasets on a GPU server via SSH/SCP.\n",
    "It handles input file transfer, inference result retrieval, emissions information retrieval, and vital sign parsing \n",
    "for multiple prompts and batch sizes.\n",
    "\n",
    "Main Functions:\n",
    "------------------\n",
    "1. Remote SSH Connection (Paramiko + SCP):\n",
    "   - `connect_with_gpu()` / `close_gpu_connection()`: Secure GPU session management.\n",
    "   - Private key-based auth to access GPU cluster.\n",
    "\n",
    "2. Extracts relevant sections from MIMIC-IV:\n",
    "   - `extract_sections(note)`: Extracts Chief Complaint, HPI, and Physical Exam from full note text.\n",
    "\n",
    "3. Note Transfer:\n",
    "   - `transfer_notes()`: \n",
    "     • Build note fragment\n",
    "     • Upload input files to the GPU server.\n",
    "     • Save local chief complaint JSON.\n",
    "\n",
    "4. Result Retrieval:\n",
    "   - `get_extracted_vitals(inference_id)`: Downloads individual output files and attempts to parse JSON outputs.\n",
    "   - `get_batch_size(inference_id)`: Repeats `get_extracted_vitals` across multiple batch sizes (1, 2, 4, 8) and aggregates emissions data.\n",
    "   - `get_emissions()`: Downloads emissions.csv file from gpu server to local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903fa8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "from scp import SCPClient\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import paramiko\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e751c09",
   "metadata": {},
   "source": [
    "# Transfer notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aec586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GPU create/close connection ===\n",
    "\n",
    "def connect_with_gpu():\n",
    "    gpu_host = \"bristol.hh.se\" #\"denver.hh.se\"\n",
    "    gpu_port = 20022\n",
    "    gpu_username = \"asirui24\"\n",
    "    private_key_path = \"C:/Users/asirui/.ssh/id_rsa\"\n",
    "\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(hostname=gpu_host, port=gpu_port, username=gpu_username, key_filename=private_key_path)\n",
    "    return ssh\n",
    "\n",
    "def close_gpu_connection(ssh):\n",
    "    try:\n",
    "        ssh.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to close SSH connection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e1e080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Extract Chief Complaint, Past Medical History and Physical Exam sections ===\n",
    "\n",
    "def extract_sections(note):\n",
    "    \n",
    "    # === Define the pattern to match the desired section ===  \n",
    "    sections = {\n",
    "        \"Chief Complaint\": [\n",
    "            (\"Chief Complaint:\", \"Major Surgical or Invasive Procedure:\"),\n",
    "            (\"___ Complaint:\", \"Major Surgical or Invasive Procedure:\")\n",
    "        ],\n",
    "        \"History of Present Illness\": [\n",
    "            (\"History of Present Illness:\", \"Past Medical History:\"),\n",
    "            (\"___ Present Illness:\", \"Past Medical History:\")\n",
    "        ],\n",
    "        \"Physical Exam\": [\n",
    "            (\"Physical Exam:\", \"Pertinent Results:\"),\n",
    "            (\"Physical ___:\", \"Pertinent Results:\"),\n",
    "            (\"Physical Exam:\", \"Brief Hospital Course:\")\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    extracted_sections = {}\n",
    "    for section_name, marker_pairs in sections.items():\n",
    "        found = False\n",
    "        for start_marker, end_marker in marker_pairs:\n",
    "            if start_marker in note and end_marker in note:\n",
    "                pattern = re.escape(start_marker) + r\"\\s*(.*?)\\s*\" + re.escape(end_marker)\n",
    "                match = re.search(pattern, note, re.DOTALL)\n",
    "                if match:\n",
    "                    extracted_sections[section_name] = match.group(1).strip()\n",
    "                    found = True\n",
    "                    break\n",
    "        if not found:\n",
    "            extracted_sections[section_name] = \"Section not found.\"\n",
    "    return extracted_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c954250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_notes():\n",
    "    \n",
    "    extracted_data = []\n",
    "\n",
    "    # === Read the contents of the Ground Truth file ===\n",
    "    with open(\"datasets/validation_dataset_GroundTruth.json\", 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # === Connect to the database ===\n",
    "    DB_NAME = \"mimic\"\n",
    "    DB_USER = \"postgres\"\n",
    "    DB_PASSWORD = \"sd98hS&GD3F4\"\n",
    "    DB_HOST = \"localhost\"\n",
    "    DB_PORT = \"5432\"\n",
    "    engine = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "\n",
    "    ssh = connect_with_gpu()\n",
    "\n",
    "    for idx, row in enumerate(data):\n",
    "        \n",
    "        note_id = row[\"note_id\"]\n",
    "        note_num = row[\"note_number\"]\n",
    "\n",
    "        query = text('''\n",
    "        SELECT text\n",
    "        FROM mimiciv_note.discharge\n",
    "        WHERE note_id = :note_id\n",
    "        ''')\n",
    "        note = pd.read_sql(query, engine, params={\"note_id\": note_id})\n",
    "        note  = note[\"text\"].values[0]\n",
    "        \n",
    "        sections = extract_sections(note)\n",
    "        cc = sections[\"Chief Complaint\"]\n",
    "        hpi = sections[\"History of Present Illness\"]\n",
    "        pe = sections[\"Physical Exam\"]\n",
    "        note_fragment = f\"History of Present Illness:\\n{hpi}\\n\\n---------------------------\\n\\nPhysical Exam:\\n{pe}\"\n",
    "\n",
    "        remote_path = \"/data/home/asirui24/validation/notes\"\n",
    "        remote_input = f\"{remote_path}/input_{note_num}.txt\"\n",
    "\n",
    "        try:\n",
    "            # === Create temp input file locally ===\n",
    "            with open(\"temp_input.txt\", \"w\") as temp_file:\n",
    "                temp_file.write(note_fragment)\n",
    "\n",
    "            # === Transfer file to remote ===\n",
    "            with SCPClient(ssh.get_transport()) as scp:\n",
    "                scp.put(\"temp_input.txt\", remote_input)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Note {note_num} transfer failed: {e}\")\n",
    "\n",
    "        record = {\n",
    "            \"note_number\": row[\"note_number\"],\n",
    "            \"note_id\": row[\"note_id\"],\n",
    "            \"chief_complaint\": cc\n",
    "        }\n",
    "\n",
    "        extracted_data.append(record)\n",
    "\n",
    "    try:\n",
    "        os.remove(\"temp_input.txt\")\n",
    "    except Exception as e:\n",
    "        print(f\"Local cleanup failed: {e}\")\n",
    "\n",
    "    with open(\"datasets/chief_complaint.json\", \"w\") as f:\n",
    "        json.dump(extracted_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    close_gpu_connection(ssh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d91b15d",
   "metadata": {},
   "source": [
    "# Get Extracted Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec3cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_parse_json(output_str):\n",
    "    try:\n",
    "        parsed = json.loads(output_str)\n",
    "        return parsed, None\n",
    "    except json.JSONDecodeError as e:\n",
    "        return {}, str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f3d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Get emissions ===\n",
    "def get_emissions(remote_path=\"/data/home/asirui24/validation/results\", local_path=\"emissions.csv\"): \n",
    "       \n",
    "    emissions_path = f\"{remote_path}/emissions.csv\" \n",
    "\n",
    "    ssh = connect_with_gpu()\n",
    "    \n",
    "    try:\n",
    "        stdin, stdout, stderr = ssh.exec_command(f\"cat {emissions_path}\")\n",
    "        emissions = stdout.read().decode()\n",
    "    except Exception as e:\n",
    "        print(f\"Emissions transfering failed: {e}\")\n",
    "        close_gpu_connection(ssh)\n",
    "\n",
    "    with open(local_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(emissions)\n",
    "\n",
    "    close_gpu_connection(ssh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c47ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Get extracted vital signs ===\n",
    "def get_extracted_vitals(inference_id):\n",
    "\n",
    "    ssh = connect_with_gpu()\n",
    "\n",
    "    remote_path = f\"/data/home/asirui24/validation/results/{inference_id}\"\n",
    "\n",
    "    with open(\"datasets/ground_truth.json\", 'r') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    with open(f\"datasets/{inference_id}.jsonl\", 'w') as f:\n",
    "\n",
    "        for idx, row in enumerate(data):\n",
    "            \n",
    "            remote_output = f\"{remote_path}/output_{idx+1}.txt\"\n",
    "\n",
    "            try:\n",
    "                stdin, stdout, stderr = ssh.exec_command(f\"cat {remote_output}\")\n",
    "                VS = stdout.read().decode()\n",
    "                if not VS.strip():\n",
    "                    print(f\"Note_{row['note_number']} - Output file is empty or missing\")\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"Note_{idx+1} failed: {e}\")\n",
    "\n",
    "            parsed_json, parse_error = try_parse_json(VS)\n",
    "\n",
    "            record = {\n",
    "                \"note_number\": row[\"note_number\"],\n",
    "                \"note_id\": row[\"note_id\"],\n",
    "                \"extracted_data\": VS,\n",
    "                \"parsed_json\": parsed_json,\n",
    "                \"parse_error\": parse_error\n",
    "            }\n",
    "            \n",
    "            f.write(json.dumps(record) + '\\n')\n",
    "            \n",
    "    close_gpu_connection(ssh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85169b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Get extracted vital signs for different batch sizes ===\n",
    "def get_batch_size(inference_id):\n",
    "    \n",
    "    ssh = connect_with_gpu()\n",
    "    \n",
    "    with open(\"datasets/ground_truth.json\", 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    emissions_out = \"emissions_batch.csv\"\n",
    "    header_written = False\n",
    "\n",
    "    for batch in [1, 2, 4, 8]:\n",
    "\n",
    "        if batch == 1:\n",
    "            remote_path = f\"/data/home/asirui24/validation/results\"\n",
    "        else:\n",
    "            remote_path = f\"/data/home/asirui24/validation/results_batch{batch}\"\n",
    "        \n",
    "        with open(f\"datasets/batch_size/{inference_id}_batch_{batch}.jsonl\", 'w') as f:\n",
    "\n",
    "            for idx, row in enumerate(data):\n",
    "                \n",
    "                remote_output = f\"{remote_path}/{inference_id}/output_{idx+1}.txt\"\n",
    "\n",
    "                try:\n",
    "                    stdin, stdout, stderr = ssh.exec_command(f\"cat {remote_output}\")\n",
    "                    VS = stdout.read().decode()\n",
    "                    if not VS.strip():\n",
    "                        print(f\"Note_{row['note_number']} - Output file is empty or missing\")\n",
    "                        continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Note_{idx+1} failed: {e}\")\n",
    "\n",
    "                parsed_json, parse_error = try_parse_json(VS)\n",
    "\n",
    "                record = {\n",
    "                    \"note_number\": row[\"note_number\"],\n",
    "                    \"note_id\": row[\"note_id\"],\n",
    "                    \"extracted_data\": VS,\n",
    "                    \"parsed_json\": parsed_json,\n",
    "                    \"parse_error\": parse_error\n",
    "                }\n",
    "                \n",
    "                f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "        # === Batch size emissions.csv ===\n",
    "\n",
    "        try:\n",
    "            stdin, stdout, stderr = ssh.exec_command(f\"cat {remote_path}/emissions.csv\")\n",
    "            emissions = stdout.read().decode()\n",
    "        except Exception as e:\n",
    "            print(f\"Emissions transfering failed: {e}\")\n",
    "            continue\n",
    "\n",
    "        temp_emissions = f\"temp_emissions.csv\"\n",
    "        with open(temp_emissions, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(emissions)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(temp_emissions)\n",
    "            filtered = df[df[\"project_name\"] == inference_id].copy()\n",
    "            filtered[\"batch_size\"] = batch  # Add batch size column\n",
    "\n",
    "            if not header_written and not filtered.empty:\n",
    "                filtered.to_csv(emissions_out, mode='w', index=False)\n",
    "                header_written = True\n",
    "            else:\n",
    "                filtered.to_csv(emissions_out, mode='a', index=False, header=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error filtering emissions: {e}\")\n",
    "        finally:\n",
    "            os.remove(temp_emissions)\n",
    "            \n",
    "    close_gpu_connection(ssh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de8214",
   "metadata": {},
   "source": [
    "# Call functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2da57ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer_notes()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ab5f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_emissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3148f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts1 = [\"llama3_10\", \"llama3_11\", \"llama3_13\", \"llama3_15\", \"llama3_20\", \"llama3_21\", \"llama3_23\", \"llama3_25\"]\n",
    "# prompts2 = [\"meditron3_10\", \"meditron3_11\", \"meditron3_13\", \"meditron3_15\", \"meditron3_20\", \"meditron3_21\", \"meditron3_23\", \"meditron3_25\"]\n",
    "# prompts3 = [\"deepseek-llm_10\", \"deepseek-llm_11\", \"deepseek-llm_13\", \"deepseek-llm_15\", \"deepseek-llm_20\", \"deepseek-llm_21\", \"deepseek-llm_23\", \"deepseek-llm_25\"]\n",
    "# prompts = prompts1 + prompts2 + prompts3\n",
    "\n",
    "# for prompt in prompts:   \n",
    "#     get_extracted_vitals(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc9a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"llama3_13\"\n",
    "# get_batch_size(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
